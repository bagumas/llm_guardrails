# LLM Guardrails
A lightweight framework for prompt-injection detection, jailbreak resilience, and ethical refusal validation.
# llm_guardrails
